{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import needest modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim \n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from VAE.model import VAE\n",
    "from progress.bar import IncrementalBar\n",
    "import matplotlib.pyplot as p\n",
    "from torch.autograd import Variable \n",
    "import matplotlib.pyplot as plt\n",
    "from VAE.data import Ego4d, DEVICE, BATCH_SIZE, transform1, transform2, ResumableRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/home/qwest/project/PycharmProjects/Reinforsment_Learning/VAE/weights/main/VAE_checkpoint_32_44.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('transform initializate sucsess')\n",
    "train_dataset = Ego4d(img_dir='/home/qwest/data_for_ml/2_25',\n",
    "                           transform1=transform1,\n",
    "                           transform2=transform2)\n",
    "print(\"train_dataset init\")\n",
    "sampler = ResumableRandomSampler(train_dataset)\n",
    "sampler.set_state(checkpoint['sampler_state'])\n",
    "print(\"train_sampler init\")\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                           batch_size=32,\n",
    "                           shuffle=False,\n",
    "                           sampler=sampler,\n",
    "                           num_workers=6)\n",
    "print(\"train_loader init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Len of trainloader: \",len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sampler.get_state(), \"test_samp.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup param's for VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 50\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model & optimizer with parametrs ^^^ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim, batch_size=BATCH_SIZE).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/home/qwest/project/PycharmProjects/Reinforsment_Learning/VAE/weights/main/VAE_checkpoint_32_44.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create train func "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "        \"\"\"\n",
    "        train VAE model.\n",
    "\n",
    "        Args:\n",
    "        epoch (int): number of epoch.\n",
    "        \"\"\"\n",
    "        bar = IncrementalBar('Countdown', max = len(train_loader))\n",
    "\n",
    "        x = next(iter(train_loader))\n",
    "        model.train()\n",
    "        print(f'Epoch {epoch} start')\n",
    "        eval_loss = 0\n",
    "        # Loop through all batches in the training dataset\n",
    "        for i, data in enumerate(train_loader):\n",
    "                data = data.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                recon_batch, mu, logvar = model(data)\n",
    "                loss = model.loss_function(recon_batch, data, mu, logvar)\n",
    "                eval_loss += loss\n",
    "                \n",
    "                loss.backward() # Compute the gradients with respect to the model parameters\n",
    "                \n",
    "                optimizer.step()\n",
    "                print(i)\n",
    "                bar.next(i) # Update the model parameters using the optimizer\n",
    "\n",
    "        torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss':loss,\n",
    "                        'epoch':epoch,\n",
    "                        'full_model':model,\n",
    "                        'sampler_state':sampler.get_state(),\n",
    "                        },\n",
    "                        f'VAE/weights/main/VAE_checkpoint_{latent_dim}_{epoch}.pt')\n",
    "        bar.finish()\n",
    "        print(f\"Avg loss: {loss:2f} \\n\")\n",
    "        model.eval()\n",
    "        recon_img, _, _ = model(x[:1].to(DEVICE))\n",
    "        img = recon_img.view(3, 64, 64).detach().cpu().numpy().transpose(1, 2, 0)\n",
    "        f = p.imshow(img)\n",
    "        p.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model with logging on mlflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log model summary.|\n",
    "with open(\"model_summary.txt\", \"w\") as f:\n",
    "    f.write(str(summary(model)))\n",
    "\n",
    "for t in range(epoch, epochs+1):\n",
    "    train(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, x.to(DEVICE), \"model.onnx\", input_names=['image'], output_names=['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('/home/qwest/project/PycharmProjects/Reinforsment_Learning/test_W/VAE_checkpoint_1283.pt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test on exist image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))\n",
    "print(x.size())\n",
    "reconstructed, mu, _ = model(x.to(DEVICE))\n",
    "reconstructed = reconstructed.view(-1, 3, 64, 64).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(reconstructed):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    plt.imshow((img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transition of image to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dog_idx = 3\n",
    "second_dog_idx = 8\n",
    "\n",
    "dz = (mu[second_dog_idx] - mu[first_dog_idx]) / 31\n",
    "walk = Variable(torch.randn(32, latent_dim, 4, 4)).to(DEVICE)\n",
    "walk[0] = mu[first_dog_idx]\n",
    "\n",
    "for i in range(1, 32):\n",
    "    walk[i] = walk[i-1] + dz\n",
    "walk = model.decoder(walk).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(walk):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    plt.imshow((img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate image from nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = Variable(torch.randn(32, latent_dim, 4, 4)).to(DEVICE)\n",
    "samples = model.decoder(samples).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "for ii, img in enumerate(samples):\n",
    "    ax = fig.add_subplot(4, 8, ii + 1, xticks=[], yticks=[])\n",
    "    plt.imshow((img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed, _, _ = model(x[0][None, :, :, :].to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = reconstructed.view(-1, 3, 64, 64).detach().cpu().numpy().transpose(0, 2, 3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04f578596da4b18aceab7ad5dfa573f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(10)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforsment-learning-cw9s_yh0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
